# ETL_challenge
The Challenge: Data Warehouse and Data Pipeline
This repository contains an end-to-end ETL pipeline that ingests, validates, transforms, and aggregates raw data files into a structured, query-ready format using Databricks.
It also includes tools for visualizing and analyzing the data model.

ğŸ§± Architecture Overview

![alt text](https://github.com/peaskipper/ETL_challenge/blob/develop/ETL%20challenge.jpg)

The pipeline is composed of the following stages:

ğŸ“¥ Ingestion

Raw pipe-delimited .gz files are read using pandas, parsed, and stored in ADLS.
External Unity Catalog (UC) tables are created with metadata and constraints.

âœ… Validation

Non-null, data type, primary key uniqueness, and foreign key relationship checks.

ğŸ“ Normalization

Hierarchy tables split into multiple normalized levels.
SQL file organises normalized table creation using logical mapping.

ğŸ“Š Aggregation (WIP)

Aggregates facts by dimensions using PySpark SQL and DLT pipelines.
Watermarking ensures incremental fault-tolerant processing.

ğŸ“ˆ Visualization

Inferred schema exported to .puml using uml_writer.py.
ER diagram rendered and saved as .png.

ğŸ“ Repository Structure
ETL_challenge/
â”œâ”€â”€ migration_library/           # Core library for parsing and ER diagram generation
â”‚   â”œâ”€â”€ ingest.py                # Reads and processes raw gz files into dataframes
â”‚   â”œâ”€â”€ analyse.py               # Extracts relationships, infers PK/FK
â”‚   â”œâ”€â”€ uml_writer.py            # Generates PlantUML format from schema
â”‚   â””â”€â”€ main.py                  # Orchestrates the extraction and parsing on local
â”‚
â”œâ”€â”€ pipeline_notebooks/         # Databricks PySpark notebooks
â”‚   â”œâ”€â”€ data_ingestion.ipynb    # Writes raw files to Unity Catalog (external tables)
â”‚   â”œâ”€â”€ data_validation.ipynb   # Data quality checks: PK, FK, nulls, type mismatches
â”‚   â”œâ”€â”€ data_staging.ipynb      # Normalizes hierarchy into separate tables
â”‚   â”œâ”€â”€ data_transformation.ipynb   # Basic transformations, joins
â”‚   â”œâ”€â”€ create_aggregate.ipynb      # Aggregation logic using SQL
â”‚   â”œâ”€â”€ data_aggregation_dlt.ipynb  # DLT-based implementation (with watermarking)
â”‚   â””â”€â”€ secret_scope.py             # Placeholder file for storing environment variables
â”‚
â”œâ”€â”€ sql_files_normalised/       # Contains SQL scripts to split hierarchies into normalised dims
â”‚   â”œâ”€â”€ dim_cat.sql
â”‚   â”œâ”€â”€ dim_subcat.sql
â”‚   â”œâ”€â”€ ...
â”‚
â”œâ”€â”€ output_obj/
â”‚   â”œâ”€â”€ ER_diagram.puml         # PlantUML source file
â”‚   â”œâ”€â”€ ERD.png                 # Generated ER diagram
â”‚   â””â”€â”€ tbl_metadata.json       # Inferred table + column metadata
â”‚
â”œâ”€â”€ source_data/                # Contains sample or placeholder gzipped input files
â”‚
â”œâ”€â”€ databricks_workflow.yaml    # Workflow YAML for Databricks job orchestration
â”œâ”€â”€ README.md                   # You're here!
â””â”€â”€ ...


ğŸš€ How to Run

1. For data ingestion and puml render on local machine, run: `ETL_challenge/migration_library/main.py`
2. For end-to-end pipeline: Set up databricks workflow using `ETL_challenge/databricks_workflow.yaml` and configure write locations

Prerequisite: Access to a Databricks workspace with Unity Catalog & ADLS mount

ğŸ“¦ Local (Setup and ER Diagram)

# Install dependencies
pip install pandas plantuml-markdown

## installations

pip install pandas  
pip install gzip
pip install graphviz
pip install pythonplantuml
pip install pyspark

## downloads
### for ER visualisation
[plantUML](https://plantuml.com/download) - standalone JAR
[java](https://www.oracle.com/java/technologies/downloads/#jdk24-windows) - prerequisite for plantUML
[puml vsc extension](https://marketplace.visualstudio.com/items?itemName=jebbs.plantuml) - for reading .puml file

## docker setup to run notebooks(pyspark, hadoop) on local 
1. Run docker
2. Use command to run pyspark on docker docker: `pull jupyter/all-spark-notebook`
3. Run docker image with this command docker: `run -it --rm -p 8888:8888 -v /path/to/your/notebooks:/home/jovyan/work jupyter/all-spark-notebook`
4. Retrieve/Update variables in notebook to point to your azure blob storage

## Set up databricks workflow using pipeline_notebooks/databricks_workflow.yaml
databricks jobs import --yaml-file databricks_workflow.yaml

ğŸ“Œ Highlights

ğŸ“š Modular design using functional libraries + parameterized notebooks
ğŸ” Automated metadata inference + schema validation
ğŸ“Š Auto-generated ERD using PlantUML

ğŸ“ Limitations

The current implementation assumes no live access to a Databricks workspace.
All Spark operations are developed and tested locally using mocks or PySpark local mode.

ğŸ“Œ Future Scope

Implement SCD Type 2 logic for slowly changing dimensions.
Integrate full CI/CD with GitHub Actions and Databricks CLI.
Extend to support Change Data Capture for near-real-time loads.

